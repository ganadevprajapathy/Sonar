{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b4W3OYp5sj9"
      },
      "source": [
        "```\n",
        "Copyright 2023 Nikolai KÃ¶rber. All Rights Reserved.\n",
        "\n",
        "Based on:\n",
        "https://keras.io/examples/generative/ddpm/\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axj3Ew8SGhvL"
      },
      "source": [
        "# IADB (Sonar dataset)\n",
        "\n",
        "This notebook demonstrates how to train an IADB diffusion model on the Sonar dataset. The overall design is inspired by https://keras.io/examples/generative/ddpm/.\n",
        "\n",
        "The official PyTorch implementation can be found [here](https://github.com/tchambon/IADB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KkDMX3jGxIt"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ZX_bCVMQ57",
        "outputId": "76e7d3e0-97aa-4eb1-a337-b35c1374242e"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/Nikolai10/Diffusion-TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxHZCazlPZ9B",
        "outputId": "0951efbb-e56b-48bd-9460-3eaaf11e299b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/Diffusion-TF/projects/IADB')\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Requires TensorFlow >=2.11 for the GroupNormalization layer.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# import tensorflow_datasets as tfds\n",
        "# from model import IADBModel\n",
        "from Diffusion_TF.projects.IADB.model import IADBModel\n",
        "from Diffusion_TF.projects.IADB.tutorials.helpers_flowers import plot_images, make_gif\n",
        "#from tutorials.helpers_flowers import plot_images, make_gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "H3K8ZHc_LyKp"
      },
      "outputs": [],
      "source": [
        "data_dir = '/home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right'\n",
        "gen_path = '/home/republic/Documents/Ganadev/Sonar/Outputs/Diffusion/Result1/Generated/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtE65P2GG0_7"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ak4RxtwNG3C9"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "#num_epochs = 1500 # e.g. 1000\n",
        "num_epochs = 1 # e.g. 1000\n",
        "norm_groups = 8  # Number of groups used in GroupNormalization layer\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# img_size = 64\n",
        "# img_height = 128\n",
        "#img_width = 800\n",
        "img_height = 128\n",
        "img_width = 800\n",
        "img_size = (img_height, img_width)\n",
        "img_channels = 1\n",
        "clip_min = -1.0\n",
        "clip_max = 1.0\n",
        "\n",
        "first_conv_channels = 64\n",
        "channel_multiplier = [1, 2, 4, 8]\n",
        "widths = [first_conv_channels * mult for mult in channel_multiplier]\n",
        "has_attention = [False, False, True, True]\n",
        "num_res_blocks = 2  # Number of residual blocks\n",
        "\n",
        "dataset_name = \"sonar\"\n",
        "splits = [\"train\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQHQMGOHD1t"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "AFgvrQVZHJMw"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# (ds,) = tfds.load(dataset_name, split=splits, with_info=False, shuffle_files=True)\n",
        "\n",
        "# ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#   data_dir,\n",
        "#   # validation_split=0.2,\n",
        "#   # subset=\"training\",\n",
        "#   seed=123,\n",
        "#   image_size=(img_height, img_width),\n",
        "#   # batch_size=batch_size,\n",
        "#   color_mode=\"grayscale\",\n",
        "#   shuffle=True,\n",
        "#   label_mode = None # Set label_mode to None to yield only images\n",
        "# )\n",
        "\n",
        "# def augment(img):\n",
        "#     \"\"\"Flips an image left/right randomly.\"\"\"\n",
        "#     return tf.image.random_flip_left_right(img)\n",
        "\n",
        "\n",
        "# def resize_and_rescale(img, size):\n",
        "#     \"\"\"Resize the image to the desired size first and then\n",
        "#     rescale the pixel values in the range [-1.0, 1.0].\n",
        "\n",
        "#     Args:\n",
        "#         img: Image tensor\n",
        "#         size: Desired image size for resizing\n",
        "#     Returns:\n",
        "#         Resized and rescaled image tensor\n",
        "#     \"\"\"\n",
        "\n",
        "#     height = tf.shape(img)[0]\n",
        "#     width = tf.shape(img)[1]\n",
        "#     crop_size = tf.minimum(height, width)\n",
        "\n",
        "#     img = tf.image.crop_to_bounding_box(\n",
        "#         img,\n",
        "#         (height - crop_size) // 2,\n",
        "#         (width - crop_size) // 2,\n",
        "#         crop_size,\n",
        "#         crop_size,\n",
        "#     )\n",
        "\n",
        "#     # Resize\n",
        "#     img = tf.cast(img, dtype=tf.float32)\n",
        "#     img = tf.image.resize(img, size=size, antialias=True)\n",
        "\n",
        "#     # Rescale the pixel values\n",
        "#     img = img / 127.5 - 1.0\n",
        "#     img = tf.clip_by_value(img, clip_min, clip_max)\n",
        "#     return img\n",
        "\n",
        "\n",
        "# def train_preprocessing(x):\n",
        "#     img = x # x is already the image tensor\n",
        "#     print(img)\n",
        "#     # img = resize_and_rescale(img, size=(img_size[0], img_size[1]))\n",
        "#     # img = augment(img)\n",
        "#     return img\n",
        "\n",
        "# for element in ds.take(1):\n",
        "#   print(\"shape\")\n",
        "#   print(element.shape)\n",
        "# train_ds = (\n",
        "#     ds.map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#     .batch(batch_size, drop_remainder=True)\n",
        "#     .shuffle(batch_size * 2)\n",
        "#     .prefetch(tf.data.AUTOTUNE)\n",
        "# )\n",
        "# # AUTOTUNE = tf.data.AUTOTUNE\n",
        "# # train_ds = ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "# print(train_ds)\n",
        "# # <_PrefetchDataset element_spec=TensorSpec(shape=(32, 64, 64, 3), dtype=tf.float32, name=None)>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QwstOqoTTQyN",
        "outputId": "7964b083-52de-47cd-f3d6-3650d0f44574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ds:\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_24.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_18.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_42.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_23.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_50.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_32.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_51.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_54.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_19.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_31.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_38.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_13.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_37.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_26.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_49.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_27.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_39.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_25.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_11.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_55.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_33.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_34.jpg\n",
            "Path: /home/republic/Documents/Ganadev/Sonar/Datasets/Data Preparation 4/mine_like_object_right/Clipboard_05-15-2024_36.jpg\n",
            "Tensor(\"resize/Squeeze:0\", shape=(128, 800, 1), dtype=float32)\n",
            "Tensor(\"ExpandDims:0\", shape=(128, 800, 1, 1), dtype=float32)\n",
            "\n",
            "After map:\n",
            "Image shape: (128, 800, 1, 1)\n",
            "\n",
            "After batch:\n",
            "Batch shape: (8, 128, 800, 1, 1)\n",
            "\n",
            "After shuffle:\n",
            "Batch shape: (8, 128, 800, 1, 1)\n",
            "\n",
            "After prefetch:\n",
            "Batch shape: (8, 128, 800, 1, 1)\n",
            "leng\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "# Assuming 'data_dir' now points to the directory with only images\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "image_files = list(data_dir.glob('*.jpg'))  # Adjust the pattern if needed\n",
        "image_files = [str(file) for file in image_files]\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=img_channels)\n",
        "    image = tf.image.resize(image, (img_height, img_width))\n",
        "    print(image)\n",
        "    #image = tf.expand_dims(image, axis=-1)\n",
        "    print(image)\n",
        "    return image\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices(image_files)\n",
        "print(\"ds:\")\n",
        "for path in ds:\n",
        "    print(\"Path:\", path.numpy().decode('utf-8'))\n",
        "train_ds = ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "print(\"\\nAfter map:\")\n",
        "for image in train_ds.take(1):  # Take one element to check\n",
        "    print(\"Image shape:\", image.shape)\n",
        "train_ds = train_ds.batch(batch_size, drop_remainder=True)\n",
        "print(\"\\nAfter batch:\")\n",
        "for batch in train_ds.take(1):\n",
        "    print(\"Batch shape:\", batch.shape)\n",
        "train_ds = train_ds.shuffle(batch_size * 2)\n",
        "print(\"\\nAfter shuffle:\")\n",
        "for batch in train_ds.take(1):\n",
        "    print(\"Batch shape:\", batch.shape)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "print(\"\\nAfter prefetch:\")\n",
        "for batch in train_ds.take(1):\n",
        "    print(\"Batch shape:\", batch.shape)\n",
        "# Now inspect the shape\n",
        "# print(\"here\")\n",
        "# print(ds.take(1))\n",
        "# for element in ds.take(1):\n",
        "#     print(element.shape)\n",
        "print(\"leng\")\n",
        "print(len(train_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB4ut02-GWJP"
      },
      "source": [
        "## Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QhxXzDBbGU8r"
      },
      "outputs": [],
      "source": [
        "# Kernel initializer to use\n",
        "def kernel_init(scale):\n",
        "    scale = max(scale, 1e-10)\n",
        "    return keras.initializers.VarianceScaling(\n",
        "        scale, mode=\"fan_avg\", distribution=\"uniform\"\n",
        "    )\n",
        "\n",
        "\n",
        "class AttentionBlock(layers.Layer):\n",
        "    \"\"\"Applies self-attention.\n",
        "\n",
        "    Args:\n",
        "        units: Number of units in the dense layers\n",
        "        groups: Number of groups to be used for GroupNormalization layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, groups=8, **kwargs):\n",
        "        self.units = units\n",
        "        self.groups = groups\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.norm = layers.GroupNormalization(groups=groups)\n",
        "        self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n",
        "        self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n",
        "        self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n",
        "        self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height = tf.shape(inputs)[1]\n",
        "        width = tf.shape(inputs)[2]\n",
        "        scale = tf.cast(self.units, tf.float32) ** (-0.5)\n",
        "\n",
        "        inputs = self.norm(inputs)\n",
        "        q = self.query(inputs)\n",
        "        k = self.key(inputs)\n",
        "        v = self.value(inputs)\n",
        "\n",
        "        attn_score = tf.einsum(\"bhwc, bHWc->bhwHW\", q, k) * scale\n",
        "        attn_score = tf.reshape(attn_score, [batch_size, height, width, height * width])\n",
        "\n",
        "        attn_score = tf.nn.softmax(attn_score, -1)\n",
        "        attn_score = tf.reshape(attn_score, [batch_size, height, width, height, width])\n",
        "\n",
        "        proj = tf.einsum(\"bhwHW,bHWc->bhwc\", attn_score, v)\n",
        "        proj = self.proj(proj)\n",
        "        return inputs + proj\n",
        "\n",
        "\n",
        "class TimeEmbedding(layers.Layer):\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.half_dim = dim // 2\n",
        "        self.emb = math.log(10000) / (self.half_dim - 1)\n",
        "        self.emb = tf.exp(tf.range(self.half_dim, dtype=tf.float32) * -self.emb)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
        "        emb = inputs[:, None] * self.emb[None, :]\n",
        "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish):\n",
        "    def apply(inputs):\n",
        "        x, t = inputs\n",
        "        input_width = x.shape[3]\n",
        "\n",
        "        if input_width == width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual = layers.Conv2D(\n",
        "                width, kernel_size=1, kernel_initializer=kernel_init(1.0)\n",
        "            )(x)\n",
        "\n",
        "        temb = activation_fn(t)\n",
        "        temb = layers.Dense(width, kernel_initializer=kernel_init(1.0))(temb)[\n",
        "            :, None, None, :\n",
        "        ]\n",
        "\n",
        "        x = layers.GroupNormalization(groups=groups)(x)\n",
        "        x = activation_fn(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(1.0)\n",
        "        )(x)\n",
        "\n",
        "        x = layers.Add()([x, temb])\n",
        "        x = layers.GroupNormalization(groups=groups)(x)\n",
        "        x = activation_fn(x)\n",
        "\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(0.0)\n",
        "        )(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def DownSample(width):\n",
        "    def apply(x):\n",
        "        x = layers.Conv2D(\n",
        "            width,\n",
        "            kernel_size=3,\n",
        "            strides=2,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init(1.0),\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def UpSample(width, interpolation=\"nearest\"):\n",
        "    def apply(x):\n",
        "        x = layers.UpSampling2D(size=2, interpolation=interpolation)(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(1.0)\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def TimeMLP(units, activation_fn=keras.activations.swish):\n",
        "    def apply(inputs):\n",
        "        temb = layers.Dense(\n",
        "            units, activation=activation_fn, kernel_initializer=kernel_init(1.0)\n",
        "        )(inputs)\n",
        "        temb = layers.Dense(units, kernel_initializer=kernel_init(1.0))(temb)\n",
        "        return temb\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    img_size,\n",
        "    img_channels,\n",
        "    widths,\n",
        "    has_attention,\n",
        "    num_res_blocks=2,\n",
        "    norm_groups=8,\n",
        "    interpolation=\"nearest\",\n",
        "    activation_fn=keras.activations.swish,\n",
        "):\n",
        "    image_input = layers.Input(\n",
        "        shape=(img_size[0], img_size[1], img_channels), name=\"image_input\"\n",
        "    )\n",
        "    print(image_input)\n",
        "    time_input = keras.Input(shape=(), dtype=tf.int64, name=\"time_input\")\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        first_conv_channels,\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_init(1.0),\n",
        "    )(image_input)\n",
        "\n",
        "    temb = TimeEmbedding(dim=first_conv_channels * 4)(time_input)\n",
        "    temb = TimeMLP(units=first_conv_channels * 4, activation_fn=activation_fn)(temb)\n",
        "\n",
        "    skips = [x]\n",
        "\n",
        "    # DownBlock\n",
        "    for i in range(len(widths)):\n",
        "        for _ in range(num_res_blocks):\n",
        "            x = ResidualBlock(\n",
        "                widths[i], groups=norm_groups, activation_fn=activation_fn\n",
        "            )([x, temb])\n",
        "            if has_attention[i]:\n",
        "                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        if widths[i] != widths[-1]:\n",
        "            x = DownSample(widths[i])(x)\n",
        "            skips.append(x)\n",
        "\n",
        "    # MiddleBlock\n",
        "    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn)(\n",
        "        [x, temb]\n",
        "    )\n",
        "    x = AttentionBlock(widths[-1], groups=norm_groups)(x)\n",
        "    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn)(\n",
        "        [x, temb]\n",
        "    )\n",
        "\n",
        "    # UpBlock\n",
        "    for i in reversed(range(len(widths))):\n",
        "        for _ in range(num_res_blocks + 1):\n",
        "            x = layers.Concatenate(axis=-1)([x, skips.pop()])\n",
        "            x = ResidualBlock(\n",
        "                widths[i], groups=norm_groups, activation_fn=activation_fn\n",
        "            )([x, temb])\n",
        "            if has_attention[i]:\n",
        "                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n",
        "\n",
        "        if i != 0:\n",
        "            x = UpSample(widths[i], interpolation=interpolation)(x)\n",
        "\n",
        "    # End block\n",
        "    x = layers.GroupNormalization(groups=norm_groups)(x)\n",
        "    x = activation_fn(x)\n",
        "    x = layers.Conv2D(3, (3, 3), padding=\"same\", kernel_initializer=kernel_init(0.0))(x)\n",
        "    return keras.Model([image_input, time_input], x, name=\"unet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYPy8zaCMtxU"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0TpN1f1ADMiU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 128, 800, 1), dtype=tf.float32, name='image_input'), name='image_input', description=\"created by layer 'image_input'\")\n",
            "<keras.src.engine.functional.Functional object at 0x7be8cd2dd240>\n",
            "(128, 800)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# Build the unet model\n",
        "network = build_model(\n",
        "    img_size=img_size,\n",
        "    img_channels=img_channels,\n",
        "    widths=widths,\n",
        "    has_attention=has_attention,\n",
        "    num_res_blocks=num_res_blocks,\n",
        "    norm_groups=norm_groups,\n",
        "    activation_fn=keras.activations.swish,\n",
        ")\n",
        "\n",
        "print(network)\n",
        "print(img_size)\n",
        "print(img_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "eFiUfVG-DOeK"
      },
      "outputs": [],
      "source": [
        "# create IADB model\n",
        "model = IADBModel(network=network)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFfwcLf7WFPY",
        "outputId": "7420fe11-0bfb-41df-f042-c29cb40f958f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_PrefetchDataset element_spec=TensorSpec(shape=(8, 128, 800, 1, 1), dtype=tf.float32, name=None)>\n",
            "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n",
            "tf.Tensor(\n",
            "[[[[[  3.]]\n",
            "\n",
            "   [[  0.]]\n",
            "\n",
            "   [[  0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 28.]]\n",
            "\n",
            "   [[ 18.]]\n",
            "\n",
            "   [[  6.]]]\n",
            "\n",
            "\n",
            "  [[[  3.]]\n",
            "\n",
            "   [[  0.]]\n",
            "\n",
            "   [[  0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 25.]]\n",
            "\n",
            "   [[ 16.]]\n",
            "\n",
            "   [[  6.]]]\n",
            "\n",
            "\n",
            "  [[[  3.]]\n",
            "\n",
            "   [[  0.]]\n",
            "\n",
            "   [[  0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 21.]]\n",
            "\n",
            "   [[ 14.]]\n",
            "\n",
            "   [[  7.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[  7.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 15.]]\n",
            "\n",
            "   [[ 25.]]\n",
            "\n",
            "   [[ 23.]]]\n",
            "\n",
            "\n",
            "  [[[  7.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 12.]]\n",
            "\n",
            "   [[ 21.]]\n",
            "\n",
            "   [[ 18.]]]\n",
            "\n",
            "\n",
            "  [[[  7.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 10.]]\n",
            "\n",
            "   [[ 17.]]\n",
            "\n",
            "   [[ 14.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[  4.]]\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 43.]]\n",
            "\n",
            "   [[ 42.]]\n",
            "\n",
            "   [[ 41.]]]\n",
            "\n",
            "\n",
            "  [[[  4.]]\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 42.]]\n",
            "\n",
            "   [[ 40.]]\n",
            "\n",
            "   [[ 39.]]]\n",
            "\n",
            "\n",
            "  [[[  4.]]\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 40.]]\n",
            "\n",
            "   [[ 37.]]\n",
            "\n",
            "   [[ 35.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[  0.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 29.]]\n",
            "\n",
            "   [[ 26.]]\n",
            "\n",
            "   [[ 30.]]]\n",
            "\n",
            "\n",
            "  [[[  0.]]\n",
            "\n",
            "   [[  2.]]\n",
            "\n",
            "   [[  2.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 28.]]\n",
            "\n",
            "   [[ 26.]]\n",
            "\n",
            "   [[ 29.]]]\n",
            "\n",
            "\n",
            "  [[[  0.]]\n",
            "\n",
            "   [[  2.]]\n",
            "\n",
            "   [[  2.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 28.]]\n",
            "\n",
            "   [[ 26.]]\n",
            "\n",
            "   [[ 29.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 15.]]\n",
            "\n",
            "   [[ 21.]]\n",
            "\n",
            "   [[ 30.]]]\n",
            "\n",
            "\n",
            "  [[[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 15.]]\n",
            "\n",
            "   [[ 21.]]\n",
            "\n",
            "   [[ 29.]]]\n",
            "\n",
            "\n",
            "  [[[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 16.]]\n",
            "\n",
            "   [[ 20.]]\n",
            "\n",
            "   [[ 28.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 22.]]\n",
            "\n",
            "   [[ 21.]]\n",
            "\n",
            "   [[ 22.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 22.]]\n",
            "\n",
            "   [[ 23.]]\n",
            "\n",
            "   [[ 25.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 22.]]\n",
            "\n",
            "   [[ 24.]]\n",
            "\n",
            "   [[ 28.]]]]\n",
            "\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            "\n",
            " [[[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 45.]]\n",
            "\n",
            "   [[ 42.]]\n",
            "\n",
            "   [[ 96.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 58.]]\n",
            "\n",
            "   [[ 56.]]\n",
            "\n",
            "   [[100.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 83.]]\n",
            "\n",
            "   [[ 87.]]\n",
            "\n",
            "   [[115.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 87.]]\n",
            "\n",
            "   [[104.]]\n",
            "\n",
            "   [[ 91.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 72.]]\n",
            "\n",
            "   [[112.]]\n",
            "\n",
            "   [[ 87.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 63.]]\n",
            "\n",
            "   [[116.]]\n",
            "\n",
            "   [[ 83.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   [[ 25.]]\n",
            "\n",
            "   [[  6.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   [[ 25.]]\n",
            "\n",
            "   [[  6.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   [[ 25.]]\n",
            "\n",
            "   [[  6.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[  6.]]\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   [[  7.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   [[ 23.]]\n",
            "\n",
            "   [[  5.]]]\n",
            "\n",
            "\n",
            "  [[[  6.]]\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   [[  7.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[  4.]]\n",
            "\n",
            "   [[ 23.]]\n",
            "\n",
            "   [[  4.]]]\n",
            "\n",
            "\n",
            "  [[[  6.]]\n",
            "\n",
            "   [[  6.]]\n",
            "\n",
            "   [[  7.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[ 23.]]\n",
            "\n",
            "   [[  4.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[118.]]\n",
            "\n",
            "   [[100.]]\n",
            "\n",
            "   [[176.]]]\n",
            "\n",
            "\n",
            "  [[[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[107.]]\n",
            "\n",
            "   [[ 86.]]\n",
            "\n",
            "   [[155.]]]\n",
            "\n",
            "\n",
            "  [[[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   [[  3.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 84.]]\n",
            "\n",
            "   [[ 66.]]\n",
            "\n",
            "   [[127.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[103.]]\n",
            "\n",
            "   [[ 79.]]\n",
            "\n",
            "   [[ 56.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 91.]]\n",
            "\n",
            "   [[ 58.]]\n",
            "\n",
            "   [[ 57.]]]\n",
            "\n",
            "\n",
            "  [[[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   [[  5.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[ 89.]]\n",
            "\n",
            "   [[ 49.]]\n",
            "\n",
            "   [[ 51.]]]]], shape=(8, 128, 800, 1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(train_ds)\n",
        "print(type(train_ds))\n",
        "# Reset the iterator of the dataset before iterating again\n",
        "train_ds_iterator = iter(train_ds)\n",
        "element = next(train_ds_iterator)\n",
        "print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho4BdtrdDScM",
        "outputId": "01b1bae7-64bc-4690-bc9b-4b939493ae9d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/home/republic/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/republic/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/republic/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/republic/Documents/Ganadev/Sonar/Models/Diffusion/Diffusion_TF/projects/IADB/model.py\", line 62, in train_step\n        x_alpha = alpha_bc * x1 + (1 - alpha_bc) * x0\n\n    ValueError: Dimensions must be equal, but are 8 and 128 for '{{node mul}} = Mul[T=DT_FLOAT](ExpandDims_2, IteratorGetNext)' with input shapes: [8,1,1,1], [8,128,800,1,1].\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model (we used a V100 GPU -> ~8s/epoch)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/tmp/__autograph_generated_file2cg9pfeo.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Ganadev/Sonar/Models/Diffusion/Diffusion_TF/projects/IADB/model.py:62\u001b[0m, in \u001b[0;36mIADBModel.train_step\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     58\u001b[0m alpha_bc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbroadcast_alpha(alpha)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# 4. Obtain x_alpha\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     x_alpha \u001b[38;5;241m=\u001b[39m \u001b[43malpha_bc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_bc) \u001b[38;5;241m*\u001b[39m x0\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# 5. Pass the interpolated images and alpha values to the network\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork([x_alpha, alpha])\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/republic/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/republic/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/republic/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/republic/Documents/Ganadev/Sonar/Models/Diffusion/Diffusion_TF/projects/IADB/model.py\", line 62, in train_step\n        x_alpha = alpha_bc * x1 + (1 - alpha_bc) * x0\n\n    ValueError: Dimensions must be equal, but are 8 and 128 for '{{node mul}} = Mul[T=DT_FLOAT](ExpandDims_2, IteratorGetNext)' with input shapes: [8,1,1,1], [8,128,800,1,1].\n"
          ]
        }
      ],
      "source": [
        "# Train the model (we used a V100 GPU -> ~8s/epoch)\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSb-wvXx3yQM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvfSuxuxOzew"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgkgeXLdqERi"
      },
      "outputs": [],
      "source": [
        "num_images = 4 * 8 # don't change this value\n",
        "print(img_size)\n",
        "print(img_channels)\n",
        "x0 = tf.random.normal(shape=(num_images, img_size[0], img_size[1], img_channels))\n",
        "print(tf.shape(x0))\n",
        "_, trajectory = model.sample_iadb(x0, nb_step=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brZZdZACpUuc"
      },
      "outputs": [],
      "source": [
        "plot_images(trajectory, gen_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcb1mucdrl5a"
      },
      "outputs": [],
      "source": [
        "make_gif(gen_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdBtN2-froVz"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(open(gen_path + 'diffusion.gif','rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xZKgWeRszP9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
